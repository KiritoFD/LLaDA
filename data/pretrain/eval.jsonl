{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training. Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence. Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines. Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization. The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization."}
{"text": "Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence. This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training. LLaDA is a diffusion-based approach to language modeling without autoregressive factorization."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training. Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence. Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization. LLaDA is a diffusion-based approach to language modeling without autoregressive factorization."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines. The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training. LLaDA is a diffusion-based approach to language modeling without autoregressive factorization."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training. Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "Training focuses on denoising masked tokens using a Transformer encoder backbone. The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence. This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines. The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "Training focuses on denoising masked tokens using a Transformer encoder backbone. The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs. The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs. Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization. LLaDA is a diffusion-based approach to language modeling without autoregressive factorization."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training. Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training. Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization."}
{"text": "Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs. The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines. Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "Training focuses on denoising masked tokens using a Transformer encoder backbone. This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence. Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "Training focuses on denoising masked tokens using a Transformer encoder backbone. The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs. Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code. The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "Diffusion-inspired language models can be trained from scratch with simple code."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization. The forward process randomly masks tokens with probability sampled per sequence."}
{"text": "Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines. The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "This tiny corpus is purely synthetic and exists only to showcase offline training. The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "Mask prediction encourages the model to recover clean text from noisy inputs. Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "LLaDA is a diffusion-based approach to language modeling without autoregressive factorization. Mask prediction encourages the model to recover clean text from noisy inputs."}
{"text": "The forward process randomly masks tokens with probability sampled per sequence. This tiny corpus is purely synthetic and exists only to showcase offline training."}
{"text": "Training focuses on denoising masked tokens using a Transformer encoder backbone."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
{"text": "The reserved mask token identifier is fixed to 126336 as described in the guidelines."}
