# LLaDA Windowsä»å¤´è®­ç»ƒæŒ‡å—

è¿™ä»½æŒ‡å—å°†å¸®åŠ©æ‚¨åœ¨Windowsä¸Šä»å¤´å¼€å§‹è®­ç»ƒLLaDAæ¨¡å‹ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç³»ç»Ÿè¦æ±‚

**æœ€ä½é…ç½®:**
- Windows 10/11
- Python 3.8+
- 6GB GPUå†…å­˜ (GTX 1660æˆ–æ›´å¥½)
- 16GB RAM
- 10GBå¯ç”¨å­˜å‚¨ç©ºé—´

**æ¨èé…ç½®:**
- Windows 11
- Python 3.9+
- 12GB+ GPUå†…å­˜ (RTX 3060æˆ–æ›´å¥½)
- 32GB RAM
- 50GBå¯ç”¨å­˜å‚¨ç©ºé—´

### 2. å®‰è£…ä¾èµ–

```bash
# å®‰è£…PyTorch (CUDAç‰ˆæœ¬)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…å…¶ä»–ä¾èµ–
pip install transformers pyyaml tqdm numpy
```

### 3. å¿«é€Ÿæµ‹è¯•

åœ¨å¼€å§‹å®Œæ•´è®­ç»ƒå‰ï¼Œå»ºè®®å…ˆè¿è¡Œå¿«é€Ÿæµ‹è¯•ï¼š

```bash
python quick_test.py
```

è¿™ä¼šè¿è¡Œä¸€ä¸ª100æ­¥çš„å°è§„æ¨¡è®­ç»ƒæ¥éªŒè¯ç¯å¢ƒé…ç½®ã€‚

## ğŸ“ è®­ç»ƒæ–¹å¼é€‰æ‹©

### æ–¹å¼1: ä¸€é”®æ‰¹å¤„ç†è„šæœ¬ (æ¨èåˆå­¦è€…)

```bash
train_windows.bat
```

è¿™ä¸ªè„šæœ¬ä¼šï¼š
- è‡ªåŠ¨æ£€æŸ¥ä¾èµ–
- ç”Ÿæˆç¤ºä¾‹æ•°æ®
- è¿è¡Œé¢„è®­ç»ƒ
- å¯é€‰æ‹©è¿è¡ŒSFTè®­ç»ƒ

### æ–¹å¼2: PowerShellè„šæœ¬

```powershell
.\training\train_windows.ps1
```

æ›´çµæ´»çš„PowerShellè„šæœ¬ï¼Œæ”¯æŒæ›´å¤šè‡ªå®šä¹‰é€‰é¡¹ã€‚

### æ–¹å¼3: Pythonè®­ç»ƒç®¡é“

```bash
python training\train_pipeline.py --config training\config_windows.yaml --stage all
```

æœ€çµæ´»çš„æ–¹å¼ï¼Œæ”¯æŒå®Œå…¨è‡ªå®šä¹‰é…ç½®ã€‚

### æ–¹å¼4: åˆ†æ­¥è®­ç»ƒ

```bash
# 1. ç”Ÿæˆæ•°æ®
python training\generate_sample_data.py

# 2. é¢„è®­ç»ƒ
python training\pretraining_from_scratch.py \
    --model_name_or_path "microsoft/DialoGPT-small" \
    --train_data_path "data\pretrain\train.jsonl" \
    --output_dir "checkpoints\pretraining" \
    --max_steps 10000 \
    --batch_size 2

# 3. SFTè®­ç»ƒ
python training\sft_training.py \
    --model_name_or_path "checkpoints\pretraining\best_model.pt" \
    --train_data_path "data\sft\train.jsonl" \
    --output_dir "checkpoints\sft" \
    --max_steps 2000

# 4. æµ‹è¯•æ¨ç†
python training\inference.py \
    --model_name_or_path "checkpoints\sft\best_sft_model.pt" \
    --prompt "Hello, how are you?"
```

## ğŸ”§ é…ç½®è¯´æ˜

### Windowsä¼˜åŒ–é…ç½® (`config_windows.yaml`)

```yaml
model:
  name_or_path: "microsoft/DialoGPT-small"  # å°æ¨¡å‹ï¼Œé€‚åˆWindows
  max_length: 1024                          # å‡å°‘å†…å­˜ä½¿ç”¨

pretraining:
  max_steps: 10000      # è¾ƒçŸ­çš„è®­ç»ƒæ­¥æ•°
  batch_size: 2         # å°æ‰¹æ¬¡å¤§å°
  num_workers: 0        # Windowså…¼å®¹æ€§
```

### å†…å­˜ä¼˜åŒ–è®¾ç½®

å¦‚æœé‡åˆ°å†…å­˜ä¸è¶³é”™è¯¯ï¼Œå°è¯•ä»¥ä¸‹è®¾ç½®ï¼š

```bash
# å‡å°‘æ‰¹æ¬¡å¤§å°
--batch_size 1

# å‡å°‘åºåˆ—é•¿åº¦
--max_length 512

# ä½¿ç”¨CPUï¼ˆå¦‚æœGPUå†…å­˜ä¸è¶³ï¼‰
set CUDA_VISIBLE_DEVICES=""
```

## ğŸ“Š è®­ç»ƒç›‘æ§

### å®æ—¶ç›‘æ§

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šåœ¨æ§åˆ¶å°è¾“å‡ºæ—¥å¿—ï¼š

```
Step: 100, Loss: 3.2456, LR: 2.98e-4
Step: 200, Loss: 3.1234, LR: 2.96e-4
Eval Loss: 3.0987
```

### æ—¥å¿—æ–‡ä»¶

è¯¦ç»†æ—¥å¿—ä¿å­˜åœ¨ï¼š
- `logs/training.log` - é¢„è®­ç»ƒæ—¥å¿—
- `logs/sft_training.log` - SFTè®­ç»ƒæ—¥å¿—

### æ£€æŸ¥ç‚¹

æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜åœ¨ï¼š
- `checkpoints/pretraining/` - é¢„è®­ç»ƒæ£€æŸ¥ç‚¹
- `checkpoints/sft/` - SFTæ£€æŸ¥ç‚¹

## ğŸ¯ å…³é”®å‚æ•°è¯´æ˜

### é¢„è®­ç»ƒå‚æ•°

- `--max_steps`: è®­ç»ƒæ­¥æ•° (Windowså»ºè®®: 5000-20000)
- `--batch_size`: æ‰¹æ¬¡å¤§å° (Windowså»ºè®®: 1-4)
- `--learning_rate`: å­¦ä¹ ç‡ (å»ºè®®: 3e-4)
- `--max_length`: åºåˆ—é•¿åº¦ (Windowså»ºè®®: 512-1024)

### SFTå‚æ•°

- `--max_steps`: SFTæ­¥æ•° (å»ºè®®: 1000-5000)
- `--learning_rate`: å­¦ä¹ ç‡ (å»ºè®®: 2e-5, æ¯”é¢„è®­ç»ƒä½)

### æ¨ç†å‚æ•°

- `--method`: é‡‡æ ·æ–¹æ³• (fixed_length/semi_autoregressive_padding)
- `--gen_length`: ç”Ÿæˆé•¿åº¦
- `--remasking`: é‡æ©ç ç­–ç•¥ (low_confidence/random)

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### GPUä¼˜åŒ–

```bash
# å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
--dtype float16

# ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯æ¨¡æ‹Ÿå¤§æ‰¹æ¬¡
--gradient_accumulation_steps 4
```

### å†…å­˜ä¼˜åŒ–

```python
# åœ¨ä»£ç ä¸­è®¾ç½®
torch.backends.cudnn.benchmark = True
torch.backends.cuda.matmul.allow_tf32 = True
```

## ğŸ› å¸¸è§é—®é¢˜è§£å†³

### 1. CUDAå†…å­˜ä¸è¶³

```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆ:**
- å‡å°‘batch_sizeåˆ°1
- å‡å°‘max_lengthåˆ°512
- å…³é—­å…¶ä»–GPUç¨‹åº

### 2. æƒé™é”™è¯¯

```
PermissionError: [WinError 5] Access is denied
```

**è§£å†³æ–¹æ¡ˆ:**
- ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ
- æ£€æŸ¥æ–‡ä»¶å¤¹æƒé™
- ä½¿ç”¨ä¸åŒçš„è¾“å‡ºç›®å½•

### 3. æ¨¡å‹åŠ è½½å¤±è´¥

```
OSError: Can't load tokenizer
```

**è§£å†³æ–¹æ¡ˆ:**
- æ£€æŸ¥ç½‘ç»œè¿æ¥
- ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
- æ¸…é™¤Hugging Faceç¼“å­˜

### 4. è®­ç»ƒä¸­æ–­

**è§£å†³æ–¹æ¡ˆ:**
- ä½¿ç”¨`--resume_from_checkpoint`æ¢å¤
- æ£€æŸ¥æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶
- ç¡®ä¿æœ‰è¶³å¤Ÿçš„å­˜å‚¨ç©ºé—´

## ğŸ“š æ•°æ®æ ¼å¼

### é¢„è®­ç»ƒæ•°æ®æ ¼å¼

```jsonl
{"text": "This is a sample text for pre-training."}
{"text": "Another piece of text for language modeling."}
```

### SFTæ•°æ®æ ¼å¼

```jsonl
{
  "conversations": [
    {"role": "user", "content": "What is AI?"},
    {"role": "assistant", "content": "AI is artificial intelligence."}
  ]
}
```

## ğŸ” è®­ç»ƒéªŒè¯

### æ£€æŸ¥è®­ç»ƒæ•ˆæœ

```bash
# æŸ¥çœ‹æŸå¤±ä¸‹é™
grep "Loss:" logs/training.log | tail -20

# æµ‹è¯•æ¨ç†
python training\inference.py \
    --model_name_or_path "checkpoints\pretraining\best_model.pt" \
    --prompt "The capital of France is" \
    --gen_length 32
```

### è¯„ä¼°æŒ‡æ ‡

- è®­ç»ƒæŸå¤±åº”è¯¥ç¨³å®šä¸‹é™
- è¯„ä¼°æŸå¤±ä¸åº”è¯¥æŒç»­ä¸Šå‡ï¼ˆè¿‡æ‹Ÿåˆï¼‰
- ç”Ÿæˆçš„æ–‡æœ¬åº”è¯¥è¿è´¯

## ğŸ“ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. é¦–å…ˆè¿è¡Œ `python quick_test.py` è¯Šæ–­ç¯å¢ƒ
2. æ£€æŸ¥ `logs/` ç›®å½•ä¸­çš„è¯¦ç»†æ—¥å¿—
3. å°è¯•å‡å°‘æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦
4. å‚è€ƒGitHub Issuesæˆ–æ–‡æ¡£

## ğŸ‰ æˆåŠŸæ ‡å¿—

è®­ç»ƒæˆåŠŸçš„æ ‡å¿—ï¼š
- âœ… æŸå¤±ç¨³å®šä¸‹é™
- âœ… ç”Ÿæˆæ£€æŸ¥ç‚¹æ–‡ä»¶
- âœ… æ¨ç†èƒ½äº§ç”Ÿåˆç†è¾“å‡º
- âœ… æ²¡æœ‰CUDAé”™è¯¯

å®Œæˆè®­ç»ƒåï¼Œæ‚¨å°±æ‹¥æœ‰äº†è‡ªå·±çš„LLaDAæ¨¡å‹ï¼